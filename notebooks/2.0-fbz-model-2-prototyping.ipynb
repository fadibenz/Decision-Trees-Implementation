{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Second Implementation of Decision Tree.\n",
    "This Notebook experiments with my second implementation of a decision tree that builds on the first draft but tries to improve performance by dealing with categorical features and missing values inside the decision tree instead of as a separate preprocessing step. "
   ],
   "id": "2c81a23af7515b8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T11:05:10.634609Z",
     "start_time": "2024-09-13T11:05:06.326462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import time \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from CART_Decision_Tree.dataset import preprocess \n",
    "from collections import Counter\n",
    "from typing import Optional, List\n",
    "import scipy.stats as stats\n",
    "from itertools import combinations\n",
    "\n"
   ],
   "id": "e674cdd6317c3e8e",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nbimporter'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mstats\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m combinations\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnbimporter\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'nbimporter'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## First improvement: Dealing with missing values on a node-specific imputation.  \n",
    "I Implemented within my decision tree a functionality to handle missing feature values based on the current node. I inferred missing values based on mode of the feature values of data points sorted\n",
    "to the current node (I chose mode to deal with categorical features).\n",
    "\n",
    "In this first implementation, missing values are inferred at each new node.\n",
    "\n",
    "NOTE: After reading more there are way more robust ways, that are very specific to the data at hand, where in each node you can have a specific imputation method, in one node you can have mean, in another median, creating a separate branch for missing values and so forth, this also can be taxing in training time and complexity. These imputation strategies should be stored and used when training and testing. "
   ],
   "id": "ab899eb93a86fbf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T11:10:53.054605Z",
     "start_time": "2024-09-13T11:10:51.015453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTreeMissing:\n",
    "    def __init__(self, max_depth: int = 3, feature_labels: Optional[List[str]]=None, max_features: Optional[int]=None,\n",
    "                 cat_cols=None):\n",
    "        if cat_cols is None:\n",
    "            cat_cols = []\n",
    "        self.cat_cols = cat_cols\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.feature_labels = feature_labels\n",
    "        self.left: Optional = None\n",
    "        self.right: Optional = None\n",
    "        self.split_idx: Optional = None\n",
    "        self.thresh: Optional =  None\n",
    "        self.prediction: Optional[int] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y: np.ndarray[int]) -> float:\n",
    "        length = len(y)\n",
    "        if length == 0:\n",
    "            return 0\n",
    "\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts[counts > 0] / length\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_o1(counter: Counter[int], labels: np.ndarray, total_count: int) -> float:\n",
    "        entropy = sum(\n",
    "            count * np.log2(count / total_count) for count in (counter[label] for label in labels) if count > 0)\n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def fill_infer(X):\n",
    "        indices = np.where(X == -1)\n",
    "        X_with_nan = np.where(X == -1, np.nan, X)\n",
    "        mode = stats.mode(X_with_nan, nan_policy='omit')\n",
    "        X_with_nan[indices] = mode[0]\n",
    "        return X_with_nan\n",
    "    \n",
    "    @staticmethod\n",
    "    def split(X: np.ndarray, y: np.ndarray[int], idx: int, thresh: float) -> tuple:\n",
    "        left_mask = X[:, idx] < thresh\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray[int]) -> 'DecisionTree':\n",
    "        nb_samples, nb_features = X.shape\n",
    "        labels = np.unique(y)\n",
    "\n",
    "        if self.max_depth == 0 or self.entropy(y) == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "            return self\n",
    "\n",
    "        best_split = {'gain': 0}\n",
    "        features_indices = np.random.choice(nb_features, size=min(self.max_features, nb_features),\n",
    "                                            replace=False) if self.max_features else range(nb_features)\n",
    "\n",
    "        for idx in features_indices:\n",
    "            X_j = X[:, idx]\n",
    "            X_j = self.fill_infer(X_j)\n",
    "            sorted_indices = np.argsort(X_j)\n",
    "            X_j_sorted, y_sorted = X_j[sorted_indices], y[sorted_indices]\n",
    "            unique_values, indices = np.unique(X_j, return_index=True)\n",
    "            y_unique = y[indices]\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "\n",
    "            y_entropy = self.entropy(y_sorted)\n",
    "            total_sum = len(y_sorted)\n",
    "\n",
    "            left_classes, right_classes = Counter(), Counter(y_sorted)\n",
    "            counts_left, counts_right = 0, total_sum\n",
    "\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                if i == 0:\n",
    "                    left_classes = Counter(y_sorted[X_j_sorted < threshold])\n",
    "                    right_classes = Counter(y_sorted[X_j_sorted >= threshold])\n",
    "                    counts_left, counts_right = sum(left_classes.values()), sum(right_classes.values())\n",
    "                else:\n",
    "                    left_classes[y_unique[i]] += 1\n",
    "                    counts_left += 1\n",
    "                    right_classes[y_unique[i]] = right_classes[y_unique[i]] - 1\n",
    "                    counts_right -= 1\n",
    "\n",
    "                weighted_entropy = (-1 / total_sum) * (\n",
    "                            self.entropy_o1(left_classes, labels, counts_left) + self.entropy_o1(right_classes, labels,\n",
    "                                                                                                 counts_right))\n",
    "                information_gain = y_entropy - weighted_entropy\n",
    "\n",
    "                if information_gain > best_split['gain']:\n",
    "                    best_split = {'gain': information_gain, 'threshold': threshold, 'feature': idx}\n",
    "\n",
    "        if best_split['gain'] == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "        else:\n",
    "            self.split_idx = best_split['feature']\n",
    "            self.thresh = best_split['threshold']\n",
    "            X_left, y_left, X_right, y_right = self.split(X, y, self.split_idx, self.thresh)\n",
    "\n",
    "            self.left = DecisionTreeMissing(self.max_depth - 1, self.feature_labels, self.max_features).fit(X_left, y_left)\n",
    "            self.right = DecisionTreeMissing(self.max_depth - 1, self.feature_labels, self.max_features).fit(X_right, y_right)\n",
    "\n",
    "        return self\n",
    "       \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        if not hasattr(self, '_imputed'):\n",
    "            for feature in range(X.shape[1]):\n",
    "                X[:, feature] = self.fill_infer(X[:, feature])\n",
    "            self._imputed = True\n",
    "        \n",
    "        if self.prediction is not None:\n",
    "            return np.full(X.shape[0], self.prediction)\n",
    "\n",
    "        predictions = np.empty(X.shape[0], dtype=int)\n",
    "        feature_values = X[:, self.split_idx]\n",
    "        left_mask = feature_values < self.thresh\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        predictions[left_mask] = self.left.predict(X[left_mask])\n",
    "        predictions[right_mask] = self.right.predict(X[right_mask])\n",
    "\n",
    "        return predictions"
   ],
   "id": "b21484dddaa080ce",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data preprocessing:",
   "id": "11d107a13d0322e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_train = '../data/raw/titanic_training.csv'\n",
    "data = np.genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "y = data[1:, 0]  # label = survived\n",
    "\n",
    "labeled_idx = np.where(y != b'')[0]\n",
    "X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[0,1, 5, 7, 8], fill_mode=False)\n",
    "\n",
    "X = X[labeled_idx, :]\n",
    "\n",
    "features = list(data[0, 1:]) + onehot_features\n",
    "df_train = pd.DataFrame(X, columns=features)\n",
    "\n",
    "df_train.to_csv('../data/interim/titanic_training_with_missing.csv', index=False)"
   ],
   "id": "9f16e344e20a4cc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validation scores:",
   "id": "c0569856a663b4cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:13:56.644371Z",
     "start_time": "2024-09-11T15:13:55.777105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.genfromtxt('../data/processed/titanic_training_cleaned.csv', delimiter=\",\", dtype=float)[1:, :]\n",
    "X_with_missing = np.genfromtxt('../data/interim/titanic_training_with_missing.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "y = np.genfromtxt('../data/processed/titanic_training_labels.csv', delimiter=\",\", dtype=int)[1:]\n",
    "\n",
    "\n",
    "print(\"sklearn's decision tree\")\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3, min_samples_leaf= 7, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "start_time = time.time()  # Record the start time\n",
    "validation_scores = cross_val_score(clf, X, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time sklearn: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nMy decision tree\")\n",
    "clf = DecisionTreeMissing(max_depth=3)\n",
    "start_time = time.time()  # Record the start time\n",
    "clf.fit(X_with_missing, y)\n",
    "validation_scores = cross_val_score(clf, X_with_missing, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")"
   ],
   "id": "ddd8048fe26e4b63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn's decision tree\n",
      "Cross validation [0.82673267 0.83663366 0.79207921 0.78712871 0.7761194 ]\n",
      "mean: 0.8037387320821633\n",
      "std: 0.023602806684993195\n",
      "Execution time sklearn: 0.0024559497833251953 seconds\n",
      "\n",
      "\n",
      "My decision tree\n",
      "Cross validation [0.83663366 0.82673267 0.78217822 0.8019802  0.7960199 ]\n",
      "mean: 0.8087089305945521\n",
      "std: 0.02007392361949834\n",
      "Execution time my implementation: 0.7439107894897461 seconds\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Results are the same as scores I got with an implementation that replaced all missing values with mode as part of preprocessing. Maybe dealing with categorical features and quantitative features differently can lead to better results. ",
   "id": "a8d0c3113aaded95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dealing with missing values with node-specific imputation with a distinction between categorical and quantitative features.  \n",
    "We inferred missing values based on mode of the feature values for categorical features and based on the mean for quantitative."
   ],
   "id": "8a21a7ac35691c20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:10:59.880539Z",
     "start_time": "2024-09-11T15:10:59.759491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTreeMissingV2(DecisionTreeMissing):\n",
    "   def __init__(self, max_depth: int = 3, feature_labels: Optional[List[str]]=None, max_features: Optional[int]=None, cat_cols = None):\n",
    "        super().__init__(max_depth, feature_labels, max_features)\n",
    "        if cat_cols is None:\n",
    "            cat_cols = []\n",
    "        self.cat_cols = cat_cols\n",
    "\n",
    "   def fill_infer_cat(self, X, idx):\n",
    "        indices = np.where(X == -1)\n",
    "        X_with_nan = np.where(X == -1, np.nan, X)\n",
    "        non_nan_values = X_with_nan[~np.isnan(X_with_nan)]              \n",
    "        \n",
    "        if len(non_nan_values) == 0:\n",
    "            return X, True\n",
    "    \n",
    "        if idx in self.cat_cols:\n",
    "          fill_with = stats.mode(non_nan_values)[0]\n",
    "        else:\n",
    "          fill_with = np.mean(non_nan_values)\n",
    "            \n",
    "        X_with_nan[indices] = fill_with\n",
    "        return X_with_nan, False \n",
    "    \n",
    "   def fit(self, X: np.ndarray, y: np.ndarray[int]) -> 'DecisionTreeMissingV2':\n",
    "        nb_samples, nb_features = X.shape\n",
    "        labels = np.unique(y)\n",
    "        if self.max_depth == 0 or self.entropy(y) == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "            return self\n",
    "\n",
    "        best_split = {'gain': 0}\n",
    "        features_indices = np.random.choice(nb_features, size=min(self.max_features, nb_features),\n",
    "                                            replace=False) if self.max_features else range(nb_features)\n",
    "\n",
    "        for idx in features_indices:\n",
    "            X_j = X[:, idx]\n",
    "            X_j, stop = self.fill_infer_cat(X_j, idx)\n",
    "            # When all values are missing I'm skipping the node\n",
    "            if stop: \n",
    "               continue\n",
    "            sorted_indices = np.argsort(X_j)\n",
    "            X_j_sorted, y_sorted = X_j[sorted_indices], y[sorted_indices]\n",
    "            unique_values, indices = np.unique(X_j, return_index=True)\n",
    "            y_unique = y[indices]\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "                \n",
    "            y_entropy = self.entropy(y_sorted)\n",
    "            total_sum = len(y_sorted)\n",
    "\n",
    "            left_classes, right_classes = Counter(), Counter(y_sorted)\n",
    "            counts_left, counts_right = 0, total_sum\n",
    "\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                if i == 0:\n",
    "                    left_classes = Counter(y_sorted[X_j_sorted < threshold])\n",
    "                    right_classes = Counter(y_sorted[X_j_sorted >= threshold])\n",
    "                    counts_left, counts_right = sum(left_classes.values()), sum(right_classes.values())\n",
    "                else:\n",
    "                    left_classes[y_unique[i]] += 1\n",
    "                    counts_left += 1\n",
    "                    right_classes[y_unique[i]] = right_classes[y_unique[i]] - 1\n",
    "                    counts_right -= 1\n",
    "\n",
    "                weighted_entropy = (-1 / total_sum) * (\n",
    "                            self.entropy_o1(left_classes, labels, counts_left) + self.entropy_o1(right_classes, labels,\n",
    "                                                                                                 counts_right))\n",
    "                information_gain = y_entropy - weighted_entropy\n",
    "                \n",
    "                left_mask = X_j < threshold\n",
    "                right_mask = ~left_mask\n",
    "    \n",
    "                if information_gain > best_split['gain']:\n",
    "                    best_split = {'gain': information_gain, 'threshold': threshold, 'feature': idx,\n",
    "                                      'left_mask': left_mask, 'right_mask': right_mask }\n",
    "                \n",
    "        if best_split['gain'] == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "        else:\n",
    "            \n",
    "            self.split_idx = best_split['feature']\n",
    "            self.thresh = best_split['threshold']\n",
    "            left_mask = best_split['left_mask']\n",
    "            right_mask = best_split['right_mask']\n",
    "            X_left, y_left, X_right, y_right = X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "            if len(y_left) == 0 :\n",
    "                print('Well',best_split['gain'])\n",
    "                print('Well',best_split['threshold'])\n",
    "            self.left = DecisionTreeMissingV2(self.max_depth - 1, self.feature_labels, self.max_features).fit(X_left, y_left)\n",
    "            self.right = DecisionTreeMissingV2(self.max_depth - 1, self.feature_labels, self.max_features).fit(X_right, y_right)\n",
    "\n",
    "        return self\n",
    "   \n",
    "   def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        if not hasattr(self, '_imputed'):\n",
    "            for feature in range(X.shape[1]):\n",
    "                X[:, feature], _ = self.fill_infer_cat(X[:, feature], feature)\n",
    "            self._imputed = True\n",
    "        \n",
    "        if self.prediction is not None:\n",
    "            return np.full(X.shape[0], self.prediction)\n",
    "\n",
    "        predictions = np.empty(X.shape[0], dtype=int)\n",
    "        feature_values = X[:, self.split_idx]\n",
    "        left_mask = feature_values < self.thresh\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        predictions[left_mask] = self.left.predict(X[left_mask])\n",
    "        predictions[right_mask] = self.right.predict(X[right_mask])\n",
    "\n",
    "        return predictions"
   ],
   "id": "fdb2b3f5e4152ae4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I encountered the problem of a node having no more meaningful values, all missing, I dealt with this by skipping the feature. There are other approaches such as having a fallback values for each feature \n",
    "\n",
    "\n",
    "NOTE: Big mistake, I was not consistent between training and prediction, When predicting values I used data that is either globally imputed, which made my implementation without meaning or predicted with data that is not imputed but did not impute it inside the predict method."
   ],
   "id": "d4aab3131cac0e65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### validation Scores:",
   "id": "cc90684baa43dec5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:11:02.676232Z",
     "start_time": "2024-09-11T15:11:02.010692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.genfromtxt('../data/processed/titanic_training_cleaned.csv', delimiter=\",\", dtype=float)[1:, :]\n",
    "X_with_missing = np.genfromtxt('../data/interim/titanic_training_with_missing.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "y = np.genfromtxt('../data/processed/titanic_training_labels.csv', delimiter=\",\", dtype=int)[1:]\n",
    "\n",
    "\n",
    "print(\"sklearn's decision tree\")\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3, min_samples_leaf= 7, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "start_time = time.time()  # Record the start time\n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time sklearn: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "validation_scores = cross_val_score(clf, X, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "\n",
    "\n",
    "print(\"\\n\\nMy decision tree\")\n",
    "clf = DecisionTreeMissingV2(max_depth=3, cat_cols= [9, 10 ,11 , 12, 13, 14, 15, 16])\n",
    "start_time = time.time()  # Record the start time\n",
    "clf.fit(X_with_missing, y) \n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")\n",
    "\n",
    "validation_scores = cross_val_score(clf, X_with_missing, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n"
   ],
   "id": "6e826c25298f12f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn's decision tree\n",
      "Execution time sklearn: 0.0 seconds\n",
      "Cross validation [0.82673267 0.83663366 0.79207921 0.78712871 0.7761194 ]\n",
      "mean: 0.8037387320821633\n",
      "std: 0.023602806684993195\n",
      "\n",
      "\n",
      "My decision tree\n",
      "Execution time my implementation: 0.08470678329467773 seconds\n",
      "Cross validation [0.80693069 0.82673267 0.79207921 0.81188119 0.80597015]\n",
      "mean: 0.8087187823259938\n",
      "std: 0.011154629178836396\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Although I thought the identical performance was due to not being distinctive between categ/quant features. It might be due to using a shallow tree. I will cross-validate to search for depth to see.",
   "id": "1e7a50ccddead236"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grid search for Hyperparameters",
   "id": "a9d498aa71778699"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:12:07.268530Z",
     "start_time": "2024-09-11T15:11:56.118271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 8],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Decision tree with mode\")\n",
    "model = DecisionTreeMissing()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best hyperparameter:\",grid_search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n Decision tree with mode/mean')\n",
    "model = DecisionTreeMissingV2()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_with_missing, y)\n",
    "print(\"Best hyperparameter:\",grid_search.best_params_)"
   ],
   "id": "8363f4a530f9cf55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with mode\n",
      "Best hyperparameter: {'max_depth': 3}\n",
      "\n",
      "\n",
      " Decision tree with mode/mean\n",
      "Best hyperparameter: {'max_depth': 3}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Conclusion:**  The model performs best with a shallow tree which means that a node-specific imputation does not improve performance. Node-specific imputation converges to a global imputation in shallow decision trees.  ",
   "id": "3688bdf698ccd809"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Second Improvement: Subset selection for categorical features.\n",
    "I implemented functionality in decision trees to determine split rules based on the subsets of categorical variables that maximize information gain"
   ],
   "id": "124f473334592486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:29:10.673305Z",
     "start_time": "2024-09-11T15:29:10.582771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTreeCategorical(DecisionTreeMissingV2):\n",
    "    def __init__(self, max_depth: int = 3, feature_labels: Optional[List[str]]=None, max_features: Optional[int]=None, cat_cols = None):\n",
    "        super().__init__(max_depth, feature_labels, max_features, cat_cols)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_subsets_combinations(arr):\n",
    "        all_subsets = []\n",
    "        for r in range(1, len(arr)):\n",
    "            subsets = combinations(arr, r)\n",
    "            all_subsets.extend(subsets)\n",
    "        return [np.array(subset) for subset in all_subsets]    \n",
    "\n",
    "\n",
    "    def information_gain(self, X, y, subset):\n",
    "        total_entropy = self.entropy(y)\n",
    "\n",
    "        left_mask = np.isin(X, subset[0])\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        y_right = y[right_mask]\n",
    "        y_left = y[left_mask]\n",
    "\n",
    "        if not len(y_right) or not len(y_left):\n",
    "            return 0, left_mask, right_mask\n",
    "\n",
    "        weighted_entropy = (len(y_right) * self.entropy(y_right) + len(y_left) * self.entropy(y_left)) / len(y)\n",
    "        return total_entropy - weighted_entropy, left_mask, right_mask\n",
    "\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray[int]) -> 'DecisionTreeCategorical':\n",
    "        nb_samples, nb_features = X.shape\n",
    "        labels = np.unique(y)\n",
    "\n",
    "        if self.max_depth == 0 or self.entropy(y) == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "            return self\n",
    "\n",
    "        best_split = {'gain': 0}\n",
    "        features_indices = np.random.choice(nb_features, size=min(self.max_features, nb_features),\n",
    "                                            replace=False) if self.max_features else range(nb_features)\n",
    "\n",
    "        for idx in features_indices:\n",
    "            X_j = X[:, idx]\n",
    "            X_j, stop = self.fill_infer_cat(X_j, idx)\n",
    "            # When all values are missing I'm skipping the node\n",
    "            if stop:\n",
    "                continue\n",
    "            sorted_indices = np.argsort(X_j)\n",
    "            X_j_sorted, y_sorted = X_j[sorted_indices], y[sorted_indices]\n",
    "            unique_values, indices = np.unique(X_j, return_index=True)\n",
    "            y_unique = y[indices]\n",
    "\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "\n",
    "            if idx in self.cat_cols:\n",
    "                subsets_combinations = self.get_subsets_combinations(unique_values)\n",
    "                n = len(subsets_combinations)\n",
    "                for index, combination in enumerate(subsets_combinations):\n",
    "                    if index == n / 2:\n",
    "                        break\n",
    "                    split = (combination, subsets_combinations[n - index - 1])\n",
    "                    information_gain_2, left_mask, right_mask = self.information_gain(X_j, y, split)   \n",
    "                    if information_gain_2 > best_split['gain']:\n",
    "                        best_split = {'gain': information_gain_2, 'threshold':split , 'feature': idx,\n",
    "                                      'left_mask': left_mask, 'right_mask': right_mask }\n",
    "            else:\n",
    "                y_entropy = self.entropy(y_sorted)\n",
    "                total_sum = len(y_sorted)\n",
    "\n",
    "                left_classes, right_classes = Counter(), Counter(y_sorted)\n",
    "                counts_left, counts_right = 0, total_sum\n",
    "\n",
    "                for i in range(len(unique_values) - 1):\n",
    "                    threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                    if i == 0:\n",
    "                        left_classes = Counter(y_sorted[X_j_sorted < threshold])\n",
    "                        right_classes = Counter(y_sorted[X_j_sorted >= threshold])\n",
    "                        counts_left, counts_right = sum(left_classes.values()), sum(right_classes.values())\n",
    "                    else:\n",
    "                        left_classes[y_unique[i]] += 1\n",
    "                        counts_left += 1\n",
    "                        right_classes[y_unique[i]] = right_classes[y_unique[i]] - 1\n",
    "                        counts_right -= 1\n",
    "\n",
    "                    weighted_entropy = (-1 / total_sum) * (\n",
    "                                self.entropy_o1(left_classes, labels, counts_left) + self.entropy_o1(right_classes, labels, counts_right))\n",
    "                    information_gain = y_entropy - weighted_entropy\n",
    "                    left_mask = X_j < threshold\n",
    "                    right_mask = ~left_mask\n",
    "\n",
    "                    if information_gain > best_split['gain']:\n",
    "                        best_split = {'gain': information_gain, 'threshold': threshold, 'feature': idx,\n",
    "                                      'left_mask': left_mask, 'right_mask': right_mask }\n",
    "\n",
    "        if best_split['gain'] == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "        else:\n",
    "            self.split_idx = best_split['feature']\n",
    "            self.thresh = best_split['threshold']\n",
    "            left_mask = best_split['left_mask']\n",
    "            right_mask = best_split['right_mask']\n",
    "            X_left, y_left, X_right, y_right = X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "            self.left = DecisionTreeCategorical(self.max_depth - 1, self.feature_labels, self.max_features).fit(X_left, y_left)\n",
    "            self.right = DecisionTreeCategorical(self.max_depth - 1, self.feature_labels, self.max_features).fit(X_right, y_right)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if not hasattr(self, '_imputed'):\n",
    "            for feature in range(X.shape[1]):\n",
    "                X[:, feature], _ = self.fill_infer_cat(X[:, feature], feature)\n",
    "            self._imputed = True\n",
    "        if self.prediction is not None:\n",
    "            return np.full(X.shape[0], self.prediction)\n",
    "\n",
    "        if isinstance(self.thresh, tuple):  # Categorical split\n",
    "            left_mask = np.isin(X[:, self.split_idx], self.thresh[0])\n",
    "        else:\n",
    "            left_mask = X[:, self.split_idx] < self.thresh\n",
    "\n",
    "        predictions = np.empty(X.shape[0], dtype=int)\n",
    "        predictions[left_mask] = self.left.predict(X[left_mask])\n",
    "        predictions[~left_mask] = self.right.predict(X[~left_mask])\n",
    "        return predictions\n"
   ],
   "id": "a346bc68d4d4a84f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../data/raw/titanic_testing_data.csv')\n",
    "\n",
    "# Select features\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "X = data[features].copy()\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "X = X.replace('', np.nan)\n",
    "\n",
    "# Encode categorical variables with LabelEncoder\n",
    "le_sex = LabelEncoder()\n",
    "le_embarked = LabelEncoder()\n",
    "\n",
    "# Fit and transform categorical features, handling NaN separately by filling with 'missing'\n",
    "X['sex'] = le_sex.fit_transform(X['sex'].fillna('missing'))\n",
    "X['embarked'] = le_embarked.fit_transform(X['embarked'].fillna('missing'))\n",
    "\n",
    "# Map 'missing' to -1 for categorical features\n",
    "if 'missing' in le_sex.classes_:\n",
    "    X['sex'] = X['sex'].replace(le_sex.transform(['missing'])[0], -1)\n",
    "if 'missing' in le_embarked.classes_:\n",
    "    X['embarked'] = X['embarked'].replace(le_embarked.transform(['missing'])[0], -1)\n",
    "\n",
    "# Fill all remaining missing values in the dataset with -1\n",
    "X = X.fillna(-1)\n",
    "\n",
    "# Convert all features to float (for compatibility)\n",
    "X = X.astype(float)\n",
    "\n",
    "# Save the processed DataFrame to CSV\n",
    "X.to_csv('../data/interim/titanic_testing_V3.csv', index=False)\n"
   ],
   "id": "bcf321868e3b0e2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:26:35.253760Z",
     "start_time": "2024-09-11T15:26:34.153058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.genfromtxt('../data/interim/titanic_training_V3.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "X_validation = np.genfromtxt('../data/processed/titanic_training_cleaned.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "y = np.genfromtxt('../data/processed/titanic_training_labels.csv', delimiter=\",\", dtype=int)[1:]\n",
    "\n",
    "print(\"\\nsklearn's decision tree\")\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3, min_samples_leaf=7, criterion='entropy')\n",
    "start_time = time.time()\n",
    "clf.fit(X, y)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "validation_scores = cross_val_score(clf, X, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "print(f\"Execution time sklearn: {execution_time} seconds\")\n",
    "\n",
    "print(\"\\nMy decision tree\")\n",
    "clf = DecisionTreeCategorical(max_depth=8, cat_cols=[0, 1, 6])\n",
    "start_time = time.time()\n",
    "clf.fit(X, y)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "validation_scores = cross_val_score(clf, X, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")"
   ],
   "id": "80d7cc9ff033e324",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sklearn's decision tree\n",
      "Cross validation [0.81188119 0.78217822 0.77722772 0.75742574 0.77114428]\n",
      "mean: 0.7799714299788187\n",
      "std: 0.01798253112472074\n",
      "Execution time sklearn: 0.020105838775634766 seconds\n",
      "\n",
      "My decision tree\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "6\n",
      "Cross validation [0.81683168 0.82178218 0.8019802  0.80693069 0.7761194 ]\n",
      "mean: 0.8047288310920646\n",
      "std: 0.01592606814699095\n",
      "Execution time my implementation: 1.0001866817474365 seconds\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5,6,  7, 8, 9, 10],\n",
    "}\n",
    "\n",
    "print('\\n\\n Decision tree categorical')\n",
    "model = DecisionTreeCategorical()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best hyperparameter:\",grid_search.best_params_)"
   ],
   "id": "979a8073d246d2a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest\n",
   "id": "67312022a816e43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:28:01.729112Z",
     "start_time": "2024-09-11T15:28:01.652495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, Tree, params: Optional[dict] = None, n: Optional[int] = 200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params: Optional[dict] = params\n",
    "        self.n: Optional[int] = n\n",
    "        self.Tree = Tree\n",
    "        self.decision_trees = [\n",
    "            Tree(**self.params) for _ in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'BaggedTrees':\n",
    "        nb_samples = X.shape[0]\n",
    "        for decisionTree in self.decision_trees:\n",
    "            indices = np.random.choice(nb_samples, size=nb_samples, replace=True)\n",
    "            decisionTree.fit(X[indices], y[indices])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        decisions = np.array([decisionTree.predict(X) for decisionTree in self.decision_trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=decisions)\n",
    "\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "    def __init__(self, Tree, params: Optional[dict] = None, n: Optional[int] = 200, m: Optional[int] = 1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.m: Optional[int] = m\n",
    "        self.n: Optional[int] = n\n",
    "        self.Tree = Tree\n",
    "        super().__init__(Tree, params, n)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        for decisionTree in self.decision_trees:\n",
    "            if hasattr(decisionTree, 'max_features'):\n",
    "                setattr(decisionTree, 'max_features', self.m)\n",
    "        return super().fit(X, y)\n"
   ],
   "id": "16e0aa104d4b1339",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T16:20:56.000999Z",
     "start_time": "2024-09-11T16:20:55.718631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.genfromtxt('../data/interim/titanic_training_V3.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "y = np.genfromtxt('../data/processed/titanic_training_labels.csv', delimiter=\",\", dtype=int)[1:]\n",
    "test = np.genfromtxt('../data/interim/titanic_testing_V3.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "\n",
    "params = {\n",
    "    'max_depth': 8, \n",
    "    'cat_cols':[0, 1, 6]\n",
    "}\n",
    "print(\"\\n Categorical Decision Tree\")\n",
    "clf = RandomForest(DecisionTreeCategorical, params, 100, 3)\n",
    "start_time = time.time()\n",
    "clf.fit(X, y)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "validation_scores = cross_val_score(clf, X, y)\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "X = np.genfromtxt('../data/processed/titanic_training_cleaned.csv', delimiter=\",\", dtype=float)[1:, :]\n",
    "\n",
    "print(\"\\n Simple Decision Tree\")\n",
    "params = {\n",
    "    'max_depth': 12\n",
    "}\n",
    "clf = RandomForest(DecisionTreeMissing, params, 300, 3)\n",
    "start_time = time.time()\n",
    "clf.fit(X, y)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "validation_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "\n",
    "print(\"Cross validation\", validation_scores)\n",
    "print('mean:' ,np.mean(validation_scores))\n",
    "print('std:', np.std(validation_scores))\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")"
   ],
   "id": "6a6b67e9fd9f9926",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Categorical Decision Tree\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random forest: Grid Search for hyperparameters ",
   "id": "b17a4369e2f39841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = np.genfromtxt('../data/interim/titanic_training_V3.csv', delimiter=\",\", dtype=float)[1:, ]\n",
    "y = np.genfromtxt('../data/processed/titanic_training_labels.csv', delimiter=\",\", dtype=int)[1:]\n",
    "\n",
    "params = {\n",
    "    'max_depth': 10, \n",
    "    'cat_cols':[0, 1, 6]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'n': [50, 100, 200],\n",
    "    'm': [3 , 7]\n",
    "}\n",
    "\n",
    "print('\\n\\n Decision tree categorical')\n",
    "model = RandomForest(DecisionTreeCategorical, params)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best hyperparameter:\",grid_search.best_params_)"
   ],
   "id": "90c6660c38265339",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
