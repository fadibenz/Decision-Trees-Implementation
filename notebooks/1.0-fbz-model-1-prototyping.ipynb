{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# First Draft of Decision Tree\n",
    "This Notebook experiments with my first implementation of a decision tree with entropy criterion. \n",
    "It uses best practices such as radix sort and O(1) entropy calculation for quantitative features.\n",
    "Categorical features are one-hot-encoded and missing values are replaced with the mode of the feature. "
   ],
   "id": "63f517cb79175a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-01T09:54:55.500326Z",
     "start_time": "2024-09-01T09:54:55.395528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numpy.ma.core import indices\n",
    "from sympy import pprint\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import time \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from typing import Optional, List\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## First Implementation of a classification Decision Tree",
   "id": "66be7535b8a4b36a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T11:26:56.269629Z",
     "start_time": "2024-09-01T11:26:56.133393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTree(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth: int = 3, feature_labels: Optional[List[str]]=None, max_features: Optional[int]=None, min_samples_leaf: int = 2,  sortfnc = None):\n",
    "        \n",
    "        self.sortfnc = sortfnc\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.feature_labels = feature_labels\n",
    "        \n",
    "        self.left: Optional['DecisionTree'] = None\n",
    "        self.right: Optional['DecisionTree'] = None\n",
    "        self.split_idx: Optional = None\n",
    "        self.thresh: Optional =  None\n",
    "        self.prediction: Optional[int] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y: np.ndarray[int]) -> float:\n",
    "        length = len(y)\n",
    "        if length == 0:\n",
    "            return 0\n",
    "\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts[counts > 0] / length\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_o1(counter: Counter[int], labels: np.ndarray, total_count: int) -> float:\n",
    "        entropy = sum(\n",
    "            count * np.log2(count / total_count) for count in (counter[label] for label in labels) if count > 0)\n",
    "        return entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def split(X: np.ndarray, y: np.ndarray[int], idx: int, thresh: float) -> tuple:\n",
    "        left_mask = X[:, idx] < thresh\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray[int]) -> 'DecisionTree':\n",
    "        nb_samples, nb_features = X.shape\n",
    "        labels = np.unique(y)\n",
    "\n",
    "        if self.max_depth == 0 or self.entropy(y) == 0 or nb_samples < self.min_samples_leaf:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "            return self\n",
    "\n",
    "        best_split = {'gain': 0}\n",
    "        features_indices = np.random.choice(nb_features, size=min(self.max_features, nb_features),\n",
    "                                            replace=False) if self.max_features else range(nb_features)\n",
    "\n",
    "        for idx in features_indices:\n",
    "            X_j = np.copy(X[:, idx])\n",
    "            if self.sortfnc: \n",
    "                sorted_indices = self.sortfnc(X_j)\n",
    "                X_j_sorted = X_j\n",
    "                y_sorted = y[sorted_indices]\n",
    "            else:\n",
    "                sorted_indices = np.argsort(X_j)\n",
    "                X_j_sorted, y_sorted = X_j[sorted_indices], y[sorted_indices]\n",
    "            \n",
    "            unique_values, indices = np.unique(X_j, return_index=True)\n",
    "            y_unique = y[indices]\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "\n",
    "            y_entropy = self.entropy(y_sorted)\n",
    "            total_sum = len(y_sorted)\n",
    "\n",
    "            left_classes, right_classes = Counter(), Counter(y_sorted)\n",
    "            counts_left, counts_right = 0, total_sum\n",
    "\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                if i == 0:\n",
    "                    left_classes = Counter(y_sorted[X_j_sorted < threshold])\n",
    "                    right_classes = Counter(y_sorted[X_j_sorted >= threshold])\n",
    "                    counts_left, counts_right = sum(left_classes.values()), sum(right_classes.values())\n",
    "                else:\n",
    "                    left_classes[y_unique[i]] += 1\n",
    "                    counts_left += 1\n",
    "                    right_classes[y_unique[i]] = right_classes[y_unique[i]] - 1\n",
    "                    counts_right -= 1\n",
    "\n",
    "                weighted_entropy = (-1 / total_sum) * (\n",
    "                            self.entropy_o1(left_classes, labels, counts_left) + self.entropy_o1(right_classes, labels,\n",
    "                                                                                                 counts_right))\n",
    "                information_gain = y_entropy - weighted_entropy\n",
    "\n",
    "                if information_gain > best_split['gain']:\n",
    "                    best_split = {'gain': information_gain, 'threshold': threshold, 'feature': idx}\n",
    "\n",
    "        if best_split['gain'] == 0:\n",
    "            self.prediction = Counter(y).most_common(1)[0][0]\n",
    "        else:\n",
    "            self.split_idx = best_split['feature']\n",
    "            self.thresh = best_split['threshold']\n",
    "            X_left, y_left, X_right, y_right = self.split(X, y, self.split_idx, self.thresh)\n",
    "\n",
    "            self.left = DecisionTree(self.max_depth - 1, self.feature_labels, self.max_features, self.min_samples_leaf).fit(X_left, y_left)\n",
    "            self.right = DecisionTree(self.max_depth - 1, self.feature_labels, self.max_features, self.min_samples_leaf).fit(X_right, y_right)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.prediction is not None:\n",
    "            return np.full(X.shape[0], self.prediction)\n",
    "\n",
    "        predictions = np.empty(X.shape[0], dtype=int)\n",
    "        feature_values = X[:, self.split_idx]\n",
    "        left_mask = feature_values < self.thresh\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        predictions[left_mask] = self.left.predict(X[left_mask])\n",
    "        predictions[right_mask] = self.right.predict(X[right_mask])\n",
    "\n",
    "        return predictions\n"
   ],
   "id": "db3268f5dc8b05c3",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross Validation and comparison of Validation scores and running time between My implementation and Sklearn's",
   "id": "efa68fd3a28cd3fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T11:27:22.750847Z",
     "start_time": "2024-09-01T11:27:22.295906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.genfromtxt('../data/processed/titanic_training_cleaned.csv', delimiter=\",\", dtype=float)[1:, :]\n",
    "y = np.genfromtxt('../data/processed/titanic_training_labels.csv', delimiter=\",\", dtype=int)[1:]\n",
    "\n",
    "\n",
    "print(\"sklearn's decision tree\")\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3, min_samples_leaf= 7, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "start_time = time.time()  # Record the start time\n",
    "print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time sklearn: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nMy decision tree\")\n",
    "clf = DecisionTree(max_depth=3, min_samples_leaf=1)\n",
    "start_time = time.time()  # Record the start time\n",
    "clf.fit(X, y)\n",
    "print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")\n"
   ],
   "id": "279fbc4c3224febc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn's decision tree\n",
      "Cross validation [0.82673267 0.83663366 0.79207921 0.78712871 0.7761194 ]\n",
      "Execution time sklearn: 0.011657238006591797 seconds\n",
      "\n",
      "\n",
      "My decision tree\n",
      "Cross validation [0.83663366 0.82673267 0.79207921 0.81188119 0.80597015]\n",
      "Execution time my implementation: 0.3010091781616211 seconds\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cross validation score is better than Sklearn's but running time is not good, my decision tree is still relatively slow, a potential improvement is to implement radix search for sorting quantitative features. But the O(1) information gain calculation dropped training time by half. An improvement for the score can be achieved with grid search to tune the hyperparameters.",
   "id": "1b28e94512eb9034"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grid search for Hyperparameters",
   "id": "176c8c62c95edd62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:04:17.913252Z",
     "start_time": "2024-09-01T10:04:08.395468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 8],\n",
    "    'min_samples_leaf': [1, 2, 7, 8, 9, 10],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"sklearn's decision tree\")\n",
    "model = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Sklearn's Best hyperparameters:\",grid_search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n My decision tree')\n",
    "model = DecisionTree()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "print(\"My tree Best hyperparameters:\",grid_search.best_params_)\n"
   ],
   "id": "557b9a06b8519496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn's decision tree\n",
      "Sklearn's Best hyperparameters: {'max_depth': 3, 'min_samples_leaf': 7}\n",
      "\n",
      "\n",
      " My decision tree\n",
      "My tree Best hyperparameters: {'max_depth': 3, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementation of radix sort.",
   "id": "cbf17ecd7cae8407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T11:26:59.915215Z",
     "start_time": "2024-09-01T11:26:59.793731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def counting_sort(arr, exp, indices):\n",
    "    n = len(arr)\n",
    "    count = np.zeros(10, dtype=int)\n",
    "    \n",
    "    # Calculate count of occurrences\n",
    "    index = (arr // exp) % 10\n",
    "    np.add.at(count, index.astype(int), 1)\n",
    "    \n",
    "    # Calculate cumulative count\n",
    "    np.cumsum(count, out=count)\n",
    "    \n",
    "    # Create output arrays\n",
    "    output = np.zeros(n, dtype=arr.dtype)\n",
    "    output_indices = np.zeros(n, dtype=int)\n",
    "    \n",
    "    # Build the output array using the cumulative count\n",
    "    for i in range(n-1, -1, -1):\n",
    "        idx = int(index[i])\n",
    "        count[idx] -= 1\n",
    "        output[count[idx]] = arr[i]\n",
    "        output_indices[count[idx]] = indices[i]\n",
    "    \n",
    "    # Copy the output array to arr and indices\n",
    "    np.copyto(arr, output)\n",
    "    np.copyto(indices, output_indices)\n",
    "\n",
    "def radix_sort(arr):\n",
    "    n = len(arr)\n",
    "    indices = np.arange(n)\n",
    "    max1 = np.max(arr)\n",
    "    exp = 1\n",
    "\n",
    "    while max1 // exp > 0:\n",
    "        counting_sort(arr, exp, indices)\n",
    "        exp *= 10\n",
    "    \n",
    "    return indices\n"
   ],
   "id": "b6c3c78d409e09ad",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T11:27:06.530516Z",
     "start_time": "2024-09-01T11:27:06.078828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"\\n\\nMy decision tree\")\n",
    "clf = DecisionTree(max_depth=3, min_samples_leaf=1, sortfnc=radix_sort)\n",
    "start_time = time.time()  # Record the start time\n",
    "clf.fit(X, y)\n",
    "print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "end_time = time.time()  # Record the end time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time my implementation: {execution_time} seconds\")"
   ],
   "id": "31de40a90526e385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My decision tree\n",
      "Cross validation [0.83663366 0.82673267 0.79207921 0.81188119 0.80597015]\n",
      "Execution time my implementation: 0.3266599178314209 seconds\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There was no real improvement.Performance stayed practically the same.",
   "id": "18247fce04be111c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "17b2bd197078c9f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
